[{"authors":["admin"],"categories":null,"content":"I am a Ph.D. candidate in the Department of Electrical and Computer Engineering at Purdue University, advised by Prof. Kaushik Roy. My research interests lie at the intersection of deep learning and edge computing. I focus on developing energy-efficient and robust deep learning algorithms, with special interests in spiking neural networks and computer vision for event-based cameras.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://chan8972.github.io/author/chankyu-lee/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chankyu-lee/","section":"authors","summary":"I am a Ph.D. candidate in the Department of Electrical and Computer Engineering at Purdue University, advised by Prof. Kaushik Roy. My research interests lie at the intersection of deep learning and edge computing.","tags":null,"title":"Chankyu Lee","type":"authors"},{"authors":["Chankyu Lee*","Syed Shakib Sarwar*","Priyadarshini Panda","Gopalakrishnan Srinivasan","Kaushik Roy (*Equal Contribution)"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"79ade04fcea75bff28e91bbe1f50fdb2","permalink":"https://chan8972.github.io/publication/2020-frontier/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/2020-frontier/","section":"publication","summary":"","tags":null,"title":"Enabling Spike-Based Backpropagation for Training Deep Neural Network Architectures","type":"publication"},{"authors":["Chankyu Lee","Adarsh Kosta","Alex Zihao Zhu","Kenneth Chaney","Kostas Daniilidis","Kaushik Roy"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"8491962f1646f70b69180307330719f3","permalink":"https://chan8972.github.io/publication/2020-eccv/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/2020-eccv/","section":"publication","summary":"","tags":null,"title":"Spike-FlowNet: Event-based Optical Flow Estimation with Energy-Efficient Hybrid Neural Networks","type":"publication"},{"authors":["Sayeed Shafayet Chowdhury*","Chankyu Lee*","Kaushik Roy (*Equal Contribution)"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"3d2f6a8460c5d957caf50e6bd283275e","permalink":"https://chan8972.github.io/publication/2020-arxiv/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/2020-arxiv/","section":"publication","summary":"Spiking Neural Networks (SNNs) are being explored to emulate the astounding capabilities of human brain that can learn and compute functions robustly and efficiently with noisy spiking activities. A variety of spiking neuron models have been proposed to resemble biological neuronal functionalities. With varying levels of bio-fidelity, these models often contain a leak path in their internal states, called membrane potentials. While the leaky models have been argued as more bioplausible, a comparative analysis between models with and without leak from a purely computational point of view demands attention. In this paper, we investigate the questions regarding the justification of leak and the pros and cons of using leaky behavior. Our experimental results reveal that leaky neuron model provides improved robustness and better generalization compared to models with no leak. However, leak decreases the sparsity of computation contrary to the common notion. Through a frequency domain analysis, we demonstrate the effect of leak in eliminating the high-frequency components from the input, thus enabling SNNs to be more robust against noisy spike-inputs.","tags":null,"title":"Towards Understanding the Effect of Leak in Spiking Neural Networks","type":"publication"},{"authors":["Chankyu Lee","Gopalakrishnan Srinivasan","Priyadarshini Panda","Kaushik Roy"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"3fef110b6b780edb3c365a011a330684","permalink":"https://chan8972.github.io/publication/2018-tcds/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/2018-tcds/","section":"publication","summary":"Spiking neural networks (SNNs) have emerged as a promising brain inspired neuromorphic-computing paradigm for cognitive system design due to their inherent event-driven processing capability. The fully connected (FC) shallow SNNs typically used for pattern recognition require large number of trainable parameters to achieve competitive classification accuracy. In this paper, we propose a deep spiking convolutional neural network (SpiCNN) composed of a hierarchy of stacked convolutional layers followed by a spatial-pooling layer and a final FC layer. The network is populated with biologically plausible leaky-integrate-and-fire (LIF) neurons interconnected by shared synaptic weight kernels. We train convolutional kernels layer-by-layer in an unsupervised manner using spike-timingdependent plasticity (STDP) that enables them to self-learn characteristic features making up the input patterns. In order to further improve the feature learning efficiency, we propose using smaller 3×3 kernels trained using STDP-based synaptic weight updates performed over a mini-batch of input patterns. Our deep SpiCNN, consisting of two convolutional layers trained using the unsupervised convolutional STDP learning methodology, achieved classification accuracies of 91.1% and 97.6%, respectively, for inferring handwritten digits from the MNIST data set and a subset of natural images from the Caltech data set.","tags":null,"title":"Deep Spiking Convolutional Neural Network Trained With Unsupervised Spike-Timing-Dependent Plasticity","type":"publication"},{"authors":["Chankyu Lee","Gopalakrishnan Srinivasan","Priyadarshini Panda","Kaushik Roy"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"9e9a7821e073aa9205b2c86bbec34c07","permalink":"https://chan8972.github.io/publication/2018-frontier/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/2018-frontier/","section":"publication","summary":"Spiking neural networks (SNNs) have emerged as a promising brain inspired neuromorphic-computing paradigm for cognitive system design due to their inherent event-driven processing capability. The fully connected (FC) shallow SNNs typically used for pattern recognition require large number of trainable parameters to achieve competitive classification accuracy. In this paper, we propose a deep spiking convolutional neural network (SpiCNN) composed of a hierarchy of stacked convolutional layers followed by a spatial-pooling layer and a final FC layer. The network is populated with biologically plausible leaky-integrate-and-fire (LIF) neurons interconnected by shared synaptic weight kernels. We train convolutional kernels layer-by-layer in an unsupervised manner using spike-timingdependent plasticity (STDP) that enables them to self-learn characteristic features making up the input patterns. In order to further improve the feature learning efficiency, we propose using smaller 3×3 kernels trained using STDP-based synaptic weight updates performed over a mini-batch of input patterns. Our deep SpiCNN, consisting of two convolutional layers trained using the unsupervised convolutional STDP learning methodology, achieved classification accuracies of 91.1% and 97.6%, respectively, for inferring handwritten digits from the MNIST data set and a subset of natural images from the Caltech data set.","tags":null,"title":"Training deep spiking convolutional neural networks with stdp-based unsupervised pre-training followed by supervised fine-tuning","type":"publication"}]