[{"authors":["admin"],"categories":null,"content":"Currently, I am a Large Scale Machine Learning Engineer at Intel Corporation. Before this, I received Ph.D. degree from School of Electrical and Computer Engineering at Purdue University, advised by Prof. Kaushik Roy. While there, I focused on developing energy-efficient and robust deep learning algorithms with special interests in spiking neural networks and computer vision for event-based cameras.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://chan8972.github.io/author/chankyu-lee/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chankyu-lee/","section":"authors","summary":"Currently, I am a Large Scale Machine Learning Engineer at Intel Corporation. Before this, I received Ph.D. degree from School of Electrical and Computer Engineering at Purdue University, advised by Prof.","tags":null,"title":"Chankyu Lee","type":"authors"},{"authors":["Chankyu Lee","Adarsh Kosta","Kaushik Roy. Arxiv Preprint, 2021"],"categories":null,"content":"","date":1616198400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616198400,"objectID":"dbaf97271e4de715509048ea02076f22","permalink":"https://chan8972.github.io/publication/2021-arxiv/","publishdate":"2021-03-20T00:00:00Z","relpermalink":"/publication/2021-arxiv/","section":"publication","summary":"Standard frame-based cameras that sample light intensity frames are heavily impacted by motion blur for high-speed motion and fail to perceive scene accurately when the dynamic range is high. Event-based cameras, on the other hand, overcome these limitations by asynchronously detecting the variation in individual pixel intensities. However, event cameras only provide information about pixels in motion, leading to sparse data. Hence, estimating the overall dense behavior of pixels is difficult. To address such issues associated with the sensors, we present Fusion-FlowNet, a sensor fusion framework for energy-efficient optical flow estimation using both frame- and event-based sensors, leveraging their complementary characteristics. Our proposed network architecture is also a fusion of Spiking Neural Networks (SNNs) and Analog Neural Networks (ANNs) where each network is designed to simultaneously process asynchronous event streams and regular frame-based images, respectively. Our network is end-to-end trained using unsupervised learning to avoid expensive video annotations. The method generalizes well across distinct environments (rapid motion and challenging lighting conditions) and demonstrates state-of-the-art optical flow prediction on the Multi-Vehicle Stereo Event Camera (MVSEC) dataset. Furthermore, our network offers substantial savings in terms of the number of network parameters and computational energy cost.","tags":null,"title":"Fusion-FlowNet: Energy-Efficient Optical Flow Estimation using Sensor Fusion and Deep Fused Spiking-Analog Network Architectures","type":"publication"},{"authors":["Chankyu Lee","Adarsh Kosta","Alex Zihao Zhu","Kenneth Chaney","Kostas Daniilidis","Kaushik Roy. In Proceedings of the European Conference on Computer Vision (ECCV) 2020"],"categories":null,"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"8491962f1646f70b69180307330719f3","permalink":"https://chan8972.github.io/publication/2020-eccv/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/2020-eccv/","section":"publication","summary":"Event-based cameras display great potential for a variety of conditions such as high-speed motion detection and enabling navigation in low-light environments where conventional frame-based cameras suffer critically. This is attributed to their high temporal resolution, high dynamic range, and low-power consumption. However, conventional computer vision methods as well as deep Analog Neural Networks (ANNs) are not suited to work well with the asynchronous and discrete nature of event camera outputs. Spiking Neural Networks (SNNs) serve as ideal paradigms to handle event camera outputs, but deep SNNs suffer in terms of performance due to spike vanishing phenomenon. To overcome these issues, we present Spike-FlowNet, a deep hybrid neural network architecture integrating SNNs and ANNs for efficiently estimating optical flow from sparse event camera outputs without sacrificing the performance. The network is end-to-end trained with self-supervised learning on Multi-Vehicle Stereo Event Camera (MVSEC) dataset. Spike-FlowNet outperforms its corresponding ANN-based method in terms of the optical flow prediction capability while providing significant computational efficiency.","tags":null,"title":"Spike-FlowNet: Event-based Optical Flow Estimation with Energy-Efficient Hybrid Neural Networks","type":"publication"},{"authors":["Chankyu Lee*","Syed Shakib Sarwar*","Priyadarshini Panda","Gopalakrishnan Srinivasan","Kaushik Roy (*Equal Contribution). Frontiers in Neuroscience, Neuromorphic Engineering, 2020"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"79ade04fcea75bff28e91bbe1f50fdb2","permalink":"https://chan8972.github.io/publication/2020-frontier/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/2020-frontier/","section":"publication","summary":"Spiking Neural Networks (SNNs) have recently emerged as a prominent neural computing paradigm. However, the typical shallow SNN architectures have limited capacity for expressing complex representations while training deep SNNs using input spikes has not been successful so far. Diverse methods have been proposed to get around this issue such as converting off-the-shelf trained deep Artificial Neural Networks (ANNs) to SNNs. However, the ANN-SNN conversion scheme fails to capture the temporal dynamics of a spiking system. On the other hand, it is still a difficult problem to directly train deep SNNs using input spike events due to the discontinuous, non-differentiable nature of the spike generation function. To overcome this problem, we propose an approximate derivative method that accounts for the leaky behavior of LIF neurons. This method enables training deep convolutional SNNs directly (with input spike events) using spike-based backpropagation. Our experiments show the effectiveness of the proposed spike-based learning on deep networks (VGG and Residual architectures) by achieving the best classification accuracies in MNIST, SVHN, and CIFAR-10 datasets compared to other SNNs trained with a spike-based learning. Moreover, we analyze sparse event-based computations to demonstrate the efficacy of the proposed SNN training method for inference operation in the spiking domain.","tags":null,"title":"Enabling Spike-Based Backpropagation for Training Deep Neural Network Architectures","type":"publication"},{"authors":["Sayeed Shafayet Chowdhury*","Chankyu Lee*","Kaushik Roy (*Equal Contribution). Arxiv Preprint, 2020"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"3d2f6a8460c5d957caf50e6bd283275e","permalink":"https://chan8972.github.io/publication/2020-arxiv/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/2020-arxiv/","section":"publication","summary":"Spiking Neural Networks (SNNs) are being explored to emulate the astounding capabilities of human brain that can learn and compute functions robustly and efficiently with noisy spiking activities. A variety of spiking neuron models have been proposed to resemble biological neuronal functionalities. With varying levels of bio-fidelity, these models often contain a leak path in their internal states, called membrane potentials. While the leaky models have been argued as more bioplausible, a comparative analysis between models with and without leak from a purely computational point of view demands attention. In this paper, we investigate the questions regarding the justification of leak and the pros and cons of using leaky behavior. Our experimental results reveal that leaky neuron model provides improved robustness and better generalization compared to models with no leak. However, leak decreases the sparsity of computation contrary to the common notion. Through a frequency domain analysis, we demonstrate the effect of leak in eliminating the high-frequency components from the input, thus enabling SNNs to be more robust against noisy spike-inputs.","tags":null,"title":"Towards Understanding the Effect of Leak in Spiking Neural Networks","type":"publication"},{"authors":["Chankyu Lee","Gopalakrishnan Srinivasan","Priyadarshini Panda","Kaushik Roy. Frontiers in Neuroscience, Neuromorphic Engineering, 2018"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"9e9a7821e073aa9205b2c86bbec34c07","permalink":"https://chan8972.github.io/publication/2018-frontier/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/2018-frontier/","section":"publication","summary":"Spiking Neural Networks (SNNs) are fast becoming a promising candidate for brain-inspired neuromorphic computing because of their inherent power efficiency and impressive inference accuracy across several cognitive tasks such as image classification and speech recognition. The recent efforts in SNNs have been focused on implementing deeper networks with multiple hidden layers to incorporate exponentially more difficult functional representations. In this paper, we propose a pre-training scheme using biologically plausible unsupervised learning, namely Spike-Timing-Dependent-Plasticity (STDP), in order to better initialize the parameters in multi-layer systems prior to supervised optimization. The multi-layer SNN is comprised of alternating convolutional and pooling layers followed by fully-connected layers, which are populated with leaky integrate-and-fire spiking neurons. We train the deep SNNs in two phases wherein, first, convolutional kernels are pre-trained in a layer-wise manner with unsupervised learning followed by fine-tuning the synaptic weights with spike-based supervised gradient descent backpropagation. Our experiments on digit recognition demonstrate that the STDP-based pre-training with gradient-based optimization provides improved robustness, faster (~2.5X) training time and better generalization compared with purely gradient-based training without pre-training.","tags":null,"title":"Training Deep Spiking Convolutional Neural Networks with STDP-based Unsupervised Pre-training Followed by Supervised Fine-tuning","type":"publication"},{"authors":["Chankyu Lee","Gopalakrishnan Srinivasan","Priyadarshini Panda","Kaushik Roy. IEEE Transactions on Cognitive and Developmental Systems (TCDS), 2018"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"3fef110b6b780edb3c365a011a330684","permalink":"https://chan8972.github.io/publication/2018-tcds/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/publication/2018-tcds/","section":"publication","summary":"Spiking neural networks (SNNs) have emerged as a promising brain inspired neuromorphic-computing paradigm for cognitive system design due to their inherent event-driven processing capability. The fully connected (FC) shallow SNNs typically used for pattern recognition require large number of trainable parameters to achieve competitive classification accuracy. In this paper, we propose a deep spiking convolutional neural network (SpiCNN) composed of a hierarchy of stacked convolutional layers followed by a spatial-pooling layer and a final FC layer. The network is populated with biologically plausible leaky-integrate-and-fire (LIF) neurons interconnected by shared synaptic weight kernels. We train convolutional kernels layer-by-layer in an unsupervised manner using spike-timingdependent plasticity (STDP) that enables them to self-learn characteristic features making up the input patterns. In order to further improve the feature learning efficiency, we propose using smaller 3×3 kernels trained using STDP-based synaptic weight updates performed over a mini-batch of input patterns. Our deep SpiCNN, consisting of two convolutional layers trained using the unsupervised convolutional STDP learning methodology, achieved classification accuracies of 91.1% and 97.6%, respectively, for inferring handwritten digits from the MNIST data set and a subset of natural images from the Caltech data set.","tags":null,"title":"Deep Spiking Convolutional Neural Network Trained With Unsupervised Spike-Timing-Dependent Plasticity","type":"publication"}]